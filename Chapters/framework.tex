%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345
%        1         2         3         4         5         6     
\chapter{Control framework}
\label{cha:framework}

Gaussian processes are a kind of supervised machine learning.
They can be seen as a generalization of the Gaussian probability distribution.
Instead of describing the probability distribution of random variables, the stochastic process governs the properties of functions.

Generally, two kinds of GPs can be distinguished.
First, classification is used where the input is mapped to discrete classes.
One well-known example is the classification of handwritten digits.
Second, regression is applied where the input is mapped to a continuous output.
Since this work uses GPs to estimate nonlinear dynamics (\cref{sec:augmentedcontroller}) the focus is on GP regression.

A quick introduction to GP classification is given in \cite{Ebden.2008b} and to GP regression in \cite{Ebden.2008}.
Modelling with GP for a tutorial example is explained in \cite{Gray.2003}.
For more information about GPs \cite{Rasmussen.2006} is proposed.

In \cref{sec:gpex} GPs are explained alongside an academic example for GP regression.
Afterwards, the focus is on two more specific parts of a GP model, the kernel (\cref{sec:covfun}) and his hyperparameters (\cref{sec:hyper}).

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Preliminaries}
\label{sec:gpex}
Gaussian process regression (GPR) is a stochastic approach to generate a function which is underlying a dataset.
It is similar to polynomial (e.g. linear) regression.
In both cases a function is returned which allows to compute the most likely values for arbitrary input values.
For GPR this function is called mean function.
The advantage of GPR is that it furthermore delivers the variance of each value which gives a hint about how reliable the value itself is.

GPR is not completely free-form because a mean and a kernel function have to be chosen leading to a prior GP.
A GP is completely specified by its mean function and covariance function.
We define the mean function $m(x)$ and the covariance function $k(x,x')$ of a process $f(x)$ as
\begin{align}
m(x) &= E[f(x)], \\
k(x,x') &= E[(f(x)-m(x))(f(x')-m(x'))],
\end{align}
where $E[f(x)]$ is the expected value and write the GP as
\begin{equation}\label{eq:GP}
f(x) \sim \mathcal{GP}(m(x),k(x,x')).
\end{equation}
For notational simplicity, we will take the mean function to be zero.
Our training data, $n$ noisy observations at the points $X$, are denoted as $\mathbf{y}$.
The function values at the $n_*$ arbitrary chosen locations $X_*$ are denoted as $\mathbf{f_*}$.
Assuming identically distributed Gaussian noise with variance $\epsilon = \sigma_n^2I$ for the measurements, the training data is described by:
\begin{equation}\label{eq:trainingdata}
\mathbf{y} = f(X)+\epsilon
\end{equation}
Therefore the prior is
\begin{equation}\label{eq:prior}
cov(\mathbf{y}) = K(X,X)+\sigma_n^2I,
\end{equation}
where the second summand corresponds the noise term. This leads to the joint distribution
\begin{equation}\label{eq:bll}
\begin{bmatrix}
\mathbf{y}\\
\mathbf{f}_*
\end{bmatrix}
\sim \mathcal{N}
\begin{pmatrix}
0, & \begin{bmatrix}
K(X,X) & K(X,X_*)\\
K(X_*,X) & K(X_*,X_*)
\end{bmatrix}
\end{pmatrix},
\end{equation}
where $K(X,X)$ is the $n \times n$ matrix containing the covariances at all pairs of training points, $K(X,X_*)$ is the $n \times n_*$ matrix containing the covarianes at all pairs of training points and test points, and similarly for $K(X_*,X)$ and $K(X_*,X_*)$.

Deriving the conditional distribution for the test points gives:
\begin{align}
\mathbf{f_*} \mid X,\mathbf{y},X_* \sim \mathcal{N}(\bar{\mathbf{f}}_*,cov(\mathbf{f_*})).
\end{align}
The most likely values for the test points $\bar{\mathbf{f_*}}$ are described by:
\begin{align}
\begin{split}
\bar{\mathbf{f}}_* &\equiv E(\mathbf{f_*} \mid X,\mathbf{y},X_*)\\
                   &= K(X_*,X)[K(X,X)+\sigma_n^2I]^{-1}\mathbf{y}
\end{split}
\end{align}
The corresponding covariances are given by:
\begin{align}
\begin{split}
cov(\mathbf{f_*}) &= K(X_*,K_*)\\
                  &-K(X_,X)[K(X,X)+\sigma_n^2I]^{-1}K(X,X_*).
\end{split}
\end{align}
Since $\bar{\mathbf{f}}_*$ is the best estimation for the function value at the test points it used in the augmented model predictive control scheme in \cref{sec:augmentedcontroller}.
$cov(\mathbf{f_*})$ is a measure for the uncertainty of the value.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Covariance functions}
\label{sec:covfun}
For the way the knowledge is transferred from the training data to the Gaussian process model, the choice of the covariance function, more generally a kernel, is crucial.
A kernel is a mapping of a pair of inputs, $\mathbf{x} \in X$, $\mathbf{x}' \in X$ into $\mathbb{R}$.
By definition, a kernel has to be symmetric, that means $k(\mathbf{x},\mathbf{x}') = k(\mathbf{x}',\mathbf{x})$.
Given a set of test points $\mathbf{x}_i$, the Gram matrix $K$ with entries $K_{ij} = k(\mathbf{x}_i,\mathbf{x}_j)$ can be computed.
If the Gram matrix $K$ fulfills
\begin{equation}\label{eq:psd}
Q(\mathbf{v})=\mathbf{v}^T K \mathbf{v} \geq 0 \quad \forall \quad \mathbf{v} \in \mathbb{R}^n,
\end{equation}
it is positive semi-definite and therefore a covariance matrix. If additionally $Q(\mathbf{v})=0$ only when $\mathbf{v}=0$, $K$ is called positive definite and is also a covariance matrix.
If a covariance function is only a function of $\mathbf{x} - \mathbf{x'}$, it is called stationary. If it is only depending on $\left| \mathbf{x} - \mathbf{x'} \right|$, it is called isotropic. One example is the standard exponential kernel
\begin{equation}\label{eq:kse}
k_{SE}\left(\mathbf{x}, \mathbf{x}'\right)= \sigma_f^2 \cdot exp \left(-\frac{\left(\mathbf{x}-\mathbf{x}'\right)^2}{2l^2}\right),
\end{equation}
where $\sigma^2$ is highest possible correlation and $l$ is the lengthscale parameter.
It is possible to add and multiply different covariance functions to generate new ones.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hyperparameters}
\label{sec:hyper}
Depending on the choice of the covariance function $k(\mathbf{x},\mathbf{x}')$, a set of hyperparameters $\theta$ has to be selected.
In case of the squared exponential kernel \ref{eq:kse} $\theta = \left\{\sigma_f^2,l\right\}$.
In most practical applications the majority of the hyperparameters are free, but sometimes fixing a hyperparameter makes sense.
For example if a periodic kernel should capture an effect related to the day-night-cycle, fixing its lengthscale to 24 hours seems appropriate.

\begin{figure}[t]
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{../Figures/long.eps}
		\caption{$l = 2.0$}
		\label{fig:exl3}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{../Figures/short.eps}
		\caption{$l = 0.2$}
		\label{fig:exl4}
	\end{subfigure}
\caption{Impact of the hyperparameters on the predictions of a GP}
\label{fig:impact_lengthscale}
\end{figure}

To determine the optimal values of the free hyperparameters, the maximum likelihood estimation is used. Given the observations $x_1$, $x_2$,..., $x_n$ and the covariance function/statistical model $f(x)$, the joint density function for all observations is defined as
\begin{equation}\label{eq:jointdensity}
f(x_1,x_2,...,x_n)=f(x_1\mid\theta)\times f(x_2\mid\theta)\times...\times f(x_n\mid\theta).
\end{equation}
By looking at this equation from a new perspective, the likelihood
\begin{equation}\label{eq:likelihood}
\mathcal{L}(\theta;x_1,x_2,...,x_n)=f(x_,x_2,...,x_n\mid\theta)=\prod_{i=1}^nf(x_i\mid\theta)
\end{equation}
is a function of the hyperparameters $\theta$.
Therefore the observations $x_i$ are fixed parameters.
Thus, the optimal hyperparameters $\theta^*$ can be derived from  
\begin{equation}\label{eq:loglikelihood}
\theta^* = \underset{\theta}{\arg\max} \quad \mathcal{L}(\theta;x_1,x_2,...,x_n).
\end{equation}
In practice, often the natural logarithm is used because this simplifies the computation of the optimal hyperparameters while giving the same results.
\cref{fig:impact_lengthscale} shows the importance of the choice of the hyperparameters. A very short lengthscale leads to massive uncertainties of the predictions.\par\medskip

The previous sections explained what GPs are and how they are completely described by the mean function and covariance function.
Moreover, the optimization of the hyperparameters and their effect on the predictions were illustrated.

MPC and GP build the theoretical base for the augmented MPC scheme provided in this work.
The following chapter derives the augmented MPC by fusing predictive and learning-based control.