%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345
%        1         2         3         4         5         6     
\tdplotsetmaincoords{70}{110}     
\chapter{Approximation based Inverse Kinematic control}
\label{cha:inverseKinematics}
Due to the non-linear natural of the kinematic system, it is very hard to apply simple controllers on it. But as mentioned before, in some regions the kinematic system can be linearized without much error \cref{subsec:switching}. Based on such fact, we proposed a strategy that linearize the kinematic modle about the current platform state at each control instance by first order approximation. Thus we gain the linear system property and simply scale down the input of the system to limit the output joint space commands so that they don't violate acceleration limits.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{First order approximation}
\label{sec:firstOrderApp}
The kinematic equations used to calculate joint commands are \cref{eq:beta},\cref{eq:betaDot} and \cref{eq:phi}. As we are focusing on the acceleration limits rather than speed limits, \cref{eq:beta} is ignored here.

To distinguish the notions for different time step, we use $\dot{\xi}^n$ to indicate the current state of the platform, while $\dot{\xi}^{n-1},\dot{\xi}^{n+1}$ corresponding to the previous and next time step command.
Through Taylor Series expansions, we linearize \cref{eq:betaDot} and \cref{eq:phi} about the current platform status $\beta, \dot{\xi}^{n}$

\begin{equation}\label{eq:firstOrderApp}
\begin{split}
\dot{\beta}_i^{n+1}&=\dot{\beta}_i^n + \Delta\dot{\beta}_i=f_{1i}(\dot{\xi}^n)(\Delta\ddot{\xi}+\ddot{\xi}^n)\\
\dot{\phi}_i^{n+1}&=\dot{\phi}_i^n + \Delta\dot{\phi}_i=f_{2i}(\beta^n)(\Delta\dot{\xi}+\dot{\xi}^n)
\end{split}
\end{equation}
Assuming the acceleration applied during a control cycle is static, from the linearized approximation, we can extract the relationship between the joint jerk and task space command:
\begin{equation}\label{eq:deltaRelation}
    \begin{split}
        \ddot{\beta}^{n+1}&=\Delta\dot{\beta}_i/\Delta T=\frac{1}{\Delta T}f_{1i}(\dot{\xi}^n)\Delta\ddot{\xi}\\
        \ddot{\phi}^{n+1}&=\Delta\dot{\phi}_i/\Delta T=\frac{1}{\Delta T}f_{2i}(\beta^n)\Delta\dot{\xi}
    \end{split}
\end{equation}


\begin{figure}
    \centering
    \begin{tikzpicture}[scale=1.5,tdplot_main_coords]      
        \draw[thick,black,->] (0,0,0) -- (3,0,0) node[anchor=north east]{$x$};       
        \draw[thick,black,->] (0,0,0) -- (0,3,0) node[anchor=north west]{$y$};       
        \draw[thick,black,->] (0,0,0) -- (0,0,3) node[anchor=south]{$\theta$};        
        \draw[black,->] (0,0,0) -- (2,2,3) node[anchor=north west]{$\dot{\xi}^n$};
        \draw[black,->] (0,0,0) -- (3,1,2) node[anchor=north east]{$\dot{\xi}^{n+1}$};
        \draw[thick,blue,->] (2,2,3) -- (3,1,2) node[anchor= west]{$\Delta\dot{\xi}$};
\end{tikzpicture}
    \caption{Caption}
    \label{fig:deltaXi}
\end{figure}  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Covariance functions}
\label{sec:covfun}
For the way the knowledge is transferred from the training data to the Gaussian process model, the choice of the covariance function, more generally a kernel, is crucial.
A kernel is a mapping of a pair of inputs, $\mathbf{x} \in X$, $\mathbf{x}' \in X$ into $\mathbb{R}$.
By definition, a kernel has to be symmetric, that means $k(\mathbf{x},\mathbf{x}') = k(\mathbf{x}',\mathbf{x})$.
Given a set of test points $\mathbf{x}_i$, the Gram matrix $K$ with entries $K_{ij} = k(\mathbf{x}_i,\mathbf{x}_j)$ can be computed.
If the Gram matrix $K$ fulfills
\begin{equation}\label{eq:psd}
Q(\mathbf{v})=\mathbf{v}^T K \mathbf{v} \geq 0 \quad \forall \quad \mathbf{v} \in \mathbb{R}^n,
\end{equation}
it is positive semi-definite and therefore a covariance matrix. If additionally $Q(\mathbf{v})=0$ only when $\mathbf{v}=0$, $K$ is called positive definite and is also a covariance matrix.
If a covariance function is only a function of $\mathbf{x} - \mathbf{x'}$, it is called stationary. If it is only depending on $\left| \mathbf{x} - \mathbf{x'} \right|$, it is called isotropic. One example is the standard exponential kernel
\begin{equation}\label{eq:kse}
k_{SE}\left(\mathbf{x}, \mathbf{x}'\right)= \sigma_f^2 \cdot exp \left(-\frac{\left(\mathbf{x}-\mathbf{x}'\right)^2}{2l^2}\right),
\end{equation}
where $\sigma^2$ is highest possible correlation and $l$ is the lengthscale parameter.
It is possible to add and multiply different covariance functions to generate new ones.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hyperparameters}
\label{sec:hyper}
Depending on the choice of the covariance function $k(\mathbf{x},\mathbf{x}')$, a set of hyperparameters $\theta$ has to be selected.
In case of the squared exponential kernel \ref{eq:kse} $\theta = \left\{\sigma_f^2,l\right\}$.
In most practical applications the majority of the hyperparameters are free, but sometimes fixing a hyperparameter makes sense.
For example if a periodic kernel should capture an effect related to the day-night-cycle, fixing its lengthscale to 24 hours seems appropriate.

\begin{figure}[t]
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{../Figures/long.eps}
		\caption{$l = 2.0$}
		\label{fig:exl3}
	\end{subfigure}
	\hfill
	\begin{subfigure}[b]{0.49\textwidth}
		\includegraphics[width=\textwidth]{../Figures/short.eps}
		\caption{$l = 0.2$}
		\label{fig:exl4}
	\end{subfigure}
\caption{Impact of the hyperparameters on the predictions of a GP}
\label{fig:impact_lengthscale}
\end{figure}

To determine the optimal values of the free hyperparameters, the maximum likelihood estimation is used. Given the observations $x_1$, $x_2$,..., $x_n$ and the covariance function/statistical model $f(x)$, the joint density function for all observations is defined as
\begin{equation}\label{eq:jointdensity}
f(x_1,x_2,...,x_n)=f(x_1\mid\theta)\times f(x_2\mid\theta)\times...\times f(x_n\mid\theta).
\end{equation}
By looking at this equation from a new perspective, the likelihood
\begin{equation}\label{eq:likelihood}
\mathcal{L}(\theta;x_1,x_2,...,x_n)=f(x_,x_2,...,x_n\mid\theta)=\prod_{i=1}^nf(x_i\mid\theta)
\end{equation}
is a function of the hyperparameters $\theta$.
Therefore the observations $x_i$ are fixed parameters.
Thus, the optimal hyperparameters $\theta^*$ can be derived from  
\begin{equation}\label{eq:loglikelihood}
\theta^* = \underset{\theta}{\arg\max} \quad \mathcal{L}(\theta;x_1,x_2,...,x_n).
\end{equation}
In practice, often the natural logarithm is used because this simplifies the computation of the optimal hyperparameters while giving the same results.
\cref{fig:impact_lengthscale} shows the importance of the choice of the hyperparameters. A very short lengthscale leads to massive uncertainties of the predictions.\par\medskip

The previous sections explained what GPs are and how they are completely described by the mean function and covariance function.
Moreover, the optimization of the hyperparameters and their effect on the predictions were illustrated.

MPC and GP build the theoretical base for the augmented MPC scheme provided in this work.
The following chapter derives the augmented MPC by fusing predictive and learning-based control.